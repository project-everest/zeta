# The high-level structure of the Veritas / Zeta

This note describes the high-level structure of the ongoing proof of
Veritas down to imperative Steel code. It also contains some notes
about how we plan to evolve this proof towards the more general Zeta
system, with support for application-specific high-integrity state
machine monitors.

Our goal for Veritas is to prove a correctness property of the
executable imperative code proccessing concrete wire-format logs. We
want to establish that if this log verifier accepts a log, then the
log entries are sequentially consistent, otherwise a hash collision
can be constructed.

# veritas-formal/high

At this level, we define the simplest, most abstract log verifier,
defined in Veritas.Verifier.fst.

The main correctness theorem at this level is in
Verias.Verifier.Correctness.fst and states:

```
val lemma_verifier_correct
    (gl: VG.hash_verifiable_log { ~ (seq_consistent (to_state_op_gvlog gl))})
  : hash_collision_gen
```

A `hash_verifiable_log` is a log on which all the high-level verifier
threads return successfully and the `add` and `evict` hashes are in
agreement.

The lemma establishes that a `hash_verifiable_log` is sequentially
consistent, otherwise a hash collision can be constructed.

This high level development serves to structure the proof of the next
level.

# veritas-formal/intermediate

The verifier at this level has one additional important detail
compared to the high-level log. In particular, rather than indexing
the store by keys (which can be very large), the verifier here uses an
indirection of "slots", short names (e.g., 16 bits) that can be bound
to a key for some duration and then released. A sub-protocol governs
the binding of slots to keys, to ensure that the binding is unique,
i.e., an active slot can refer to exactly one key.

This verifier is defined in Veritas.Intermediate.Verify.fsti

The main correctness theorem is in
Veritas.Intermediate.Correctness.fsti, and is similar to the
high-level correctness statement

```
let lemma_verifier_correct
       (#vcfg:_)
       (gl: IntG.hash_verifiable_log vcfg { ~ (seq_consistent (IntG.to_state_ops gl))})
  : hash_collision_gen =
```

Exvept, this time `IntG.hash_verifiable_log` is a statement about logs
successfully verified by the intermediate verifier. Again, if those
verifiers agree, then a log is sequentially consistent, otherwise a
hash collision can be constructed.

The proof of this lemma makes use of the high-level lemma. From here
on below, the high-level lemma is no longer relevant. We'll only use
the intermediate one.

From here on down, the proofs are just about functional correctness
refinement. All the semantic proofs of the protocol, reasoning about
hashes and concurrency etc., are done.

## Some TODOS

* Epochs: I am not sure of the current status of support for verifier
  epochs.

# veritas-formal/low

This level doesn't exist. Aymeric is defining it.

At this level, we should define Veritas.Low.Verifier.fst, a total
function whose type is

```

val verify (vcfg:_) (tsm:thread_state_model) (entries:seq vlog_entry)
   : thread_state_model
```

This is a total function meant as a low-level specification of the
Steel layer below. It uses the same data structures as the Steel
program, notably `thread_state_model` and `vlog_entry`, the latter the
type of log entries generated by EverParse.

## Proof of per-thread equivalence / simulation

The main proof at this level is a lemma relating
Veritas.Low.Verifier.verify to Veritas.Intermediate.Verifier.verify.

Both of these functions represent a single verifier thread's batch
processing of a log of entries assigned to that thread.

These are pure F* proofs, relating two total state-transforming
functions and the states on which they operate.

Specifically, we need the following:

1. State relation: `thread_state_model ~ vtls vcfg`

   A `thread_state_model` is the Low verifier's state

   A `vtls vcfg` is the intermediate verifier's state
     - The `vcfg` is a parameter that defines the store size
       The low verifier does not have such a parameter yet, it probably should.

   The state relation looks something like this:

```
let related (tsm:thread_state_model) #vcfg (vtls:vtls vcfg) =
    VCache.size (tsm.model_store) == vcfg.store_size /\
    (if tsm.model_failed then Failed? vtls
     else (
       Valid? vtls /\
       let Valid id st clock hadd hevict = vtls in
       id == ??? // thread_state_model doesn't have a thread id yet
       store_related tsm.model_store st /\
       clock_related tsm.model_clock clock /\
       hash_related tsm.model_hadd hadd /\
       hash_related tsm.model_hevict hevice
     )
```

3. A function `lift` to relate log entries

   The Low verifier operators on `seq vlog_entry`
   The intermediate verifier operates on `logS_entry`

   We need a ghost function, `lift : vlog_entry -> logS_entry`

   And an easy lemma about it, to show that it correctly preserves all
   application-facing entries in the log.

```
forall entries. state_ops' entries == state_ops (map lift entries)
```


4. Low and intermediate runs are related when run on related states

```
forall vcfg (vtls:vtls vcfg) (tsm:_) (entries:seq vlog_entry).
    tsm `related` vtls ==>
    Low.verify tsm entries `related`
    Intermediate.verify vtls (map lift entries)
```

# veritas-formal/steel

Finally, we have a Steel implementation of Low.verify. This is the
imperative code that we extract to C.

We have a two versions of this code written and verified already. But,
we are now attempting another, improved version

One main consideration at this level is the change in log
format. Rather than operating on a sequence of log entries, this
version operates on a binary representation of those log entries
represented in an array of bytes and processed by EverParse.

Here's its type:

```
let verify (vs:thread_state_t) (#n:U32.t) (log:array n U8.t)
  : SteelSel bool
    (expects
       thread_state_inv vs * // abstract invariant of a verifier thread
       readable log          // read permission on the input array
    )
    (provides fun _ ->
       thread_state_inv vs * // restores the same
       readable log
    )
    (requires fun h ->
        True)
    (ensures fun h0 format_ok h1 ->
        //one more bit to record a failure mode not present at the higher levels
        format_ok == valid_log_bytes (bytes_of log h0) /\
        if format_ok  //if the log is well formatted
        then (//it computes the same function as Low.verify
         let entries = parse_bytes (bytes_of log h0) in
         v_thread vs h1 == Low.verify (v_thread vs h0) entries
        )
     )
```

# Top-level multi-threaded statement

So far, we have established that the each Steel verifier thread
behaves equivalently to each Intermediate level verifier thread.

But, the top-level correctness statement is about multiple verifier
threads processing a clock-sorted interleaved log.

We would like to have a final top-level statement about a
multi-threaded Steel verifier that says that if it accepts a log, then
that log must be sequentially consistent.

To get there, we first have to decide how the threads are set up.

## Interactivity

Rather than processing all log entries in batch mode, in practice we
want a more interactive/incremental verifier. An application thread
continuously pushes log entries to a verifier thread, and
periodically, the verifier signals that it has completed processing a
batch of entries belonging to an epoch.

To enable this, we envision the following top-level orchestration of
verifier and application threads.

1. The application launches a verifier main thread with a parameter
   `n` specifying the number of worker threads.

2. Main thread spawns `n` worker threads and opens two bidirectional
   channels with each of them:

   - App-Worker channel:

     One endpoint of this channel is given to the worker thread, the
     other endpoint is returned by the main thread to the application.

     This channel is used to communicate log entries from the
     application threads directly to the worker threads.

   - Main-Worker channel:

     The main thread communicates with all the workers through these n
     channels.

     At the end of an epoch, each worker thread sends its add and
     evict hashes (if it hasn't failed) to the main thread, which then
     aggregates and compares the hashes from all the threads and
     signals back to the application if an epoch is deemed
     sequentially consistent.

## An invariant sketch for the top-level theorem

The top-level theorem should apply at a point where the main verifier
thread has checked all the hashes from the worker threads for a given
epoch.

How would that look? Here's a rough sketch.


1. Main verifier has state to maintain the thread epoch hashes

```
val teh : array n (epoch_hashes)
```

i.e, one `epoch_hashes` entry for each of the `n` threads, where

The `epoch_hashes` structure is a partial map from the epoch IDs to
the computed hashes for that epoch. The number of epochs is unbounded,
but in practice Zeta enforces that the maximum number of
as-yet-unverified epochs is small (e.g., 16). But, for now, I'm going
to ignore that and just represent the `epoch_hashes` abstractly as a
pure infinite map.

```
let epoch_hash_entry =
    (entries:erased (seq vlog_entry){ received_from_app entries }  &
     signature: sig (state_ops entries) &
     hadd:hash &
     hevict:hash {
       let final = Low.verify (init i) entries in
       final.ok /\
       final.hadd = hadd /\
       final.hevict = hevict
     })
let epoch_hashes =
    epoch_id -> option epoch_hash_entry
```

I.e. For each epoch, a worker thread can register in the main thread a few things:

- `entries`: An erased sequence of log entries with a refinement that
  these are the entries received from the app

- `signature`: A digital signature under the verifier's key attesting
  to all the state_ops in the entries

- `hadd` and `hevict`, with a refinement stating that these values are
  the result of successfully running the low verifier from the initial
  state for that thread for that epoch on the entries list.

2. Additionally, the main verifier maintains some state to notice when
   all the epoch hashes for a given epoch have been populated

   When that happens, the main verifier aggregates all the hadds and
   hevicts, checks that they cancel each other, and from the
   refinements, can conclude that

   For a clock-sorted interleaving of the erased `seq vlog_entry` for
   each thread, all the Low verifier threads succeeded, and their hash
   check also succeeded,

   So, from the simulation lemma, we can conclude that the
   intermediate verifiers would also succeed and their hash checks
   would also succeed, and hence the state_ops of the lifting of the
   clock-sorted interleaving of the vlog entries are sequentially
   consistent.

   However, since the state ops of the lifted entries are in relation
   with the state_ops of the `vlog_entries`, we can conclude that the
   low-level state ops are also sequentially consistent.

## Channel specifications to maintain the invariant

1. First allocate the App-Worker channel

   It's high-level type looks (from the perspective of the worker
   thread) something like:


   ```
   let rec loop () =
      do_while (b <-- recv bool; return b) //App signals whether to continue
               (
                entries <-- recv (seq vlog_entry); //wait to receive some entries
                send unit //acknowledge receipt and loop
               )

   ```

2. Define `received_from_app entries` as a property of the trace of
   the App-Worker channel: It states that the entries are recorded in
   the history of the channel.

3. Then allocate the Main-Worker channel, which looks something like
   this, from the perspective of the Main thread

    ```
    let rec loop () =
        do_while (b <-- recv bool; return b) //the worker thread signals whether to continue
                 (
                   _ <-- recv (epoch_id & epoch_hash_entry);

                   send unit //ack
                 )

    ```

--------------------------------------------------------------------------------
